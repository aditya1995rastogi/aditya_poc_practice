{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5776f1-3ddf-4f9a-b83a-5120e147d8a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pypdf langchain langchain-text-splitters\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af0889a-7ebe-4937-8647-5942f2a3168a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c48116-1cff-4a52-bc97-a43ab4a2003c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_existing_file = spark.sql(\"select distinct file_name from workspace.input_data.drug_file_tracker\")\n",
    "existing_files = [row['file_name'] for row in df_existing_file.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf05ba4-7840-4cef-858c-279e597157dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Folder path\n",
    "uc_location_path = \"/Volumes/workspace/input_data/pdf_drugs/\"\n",
    "\n",
    "# Recursive splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Iterating through the files\n",
    "for file in os.listdir(uc_location_path): \n",
    "    if file.endswith(\".pdf\"):\n",
    "        if file not in existing_files:\n",
    "\n",
    "            spark.sql(f\"\"\"INSERT INTO workspace.input_data.drug_file_tracker (file_name)VALUES ('{file}')\"\"\")\n",
    "\n",
    "            file_path = os.path.join(uc_location_path, file)\n",
    "            file_name = file\n",
    "\n",
    "            # Extract drug name from filename\n",
    "            drug_name = file_name.split(\".\")[0]\n",
    "            print(f\"Processing: {file_name}\")\n",
    "        \n",
    "            # Read PDF\n",
    "            reader = PdfReader(file_path)\n",
    "        \n",
    "            full_text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text + \"\\n\"\n",
    "        \n",
    "            # Skip empty documents\n",
    "            if not full_text.strip():\n",
    "                print(f\"Skipped empty file: {file_name}\")\n",
    "                continue\n",
    "        \n",
    "            # Recursive chunking\n",
    "            chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "            # Prepare dataframe\n",
    "            current_time = datetime.now()\n",
    "            data = [(file_name, drug_name, chunk, current_time) for chunk in chunks]\n",
    "        \n",
    "            df = spark.createDataFrame(\n",
    "                data,\n",
    "                [\"file_name\", \"drug_name\", \"chunk_text\", \"created_at\"]\n",
    "            )\n",
    "        \n",
    "            # Append to Delta table\n",
    "            df.write.mode(\"append\").saveAsTable(\"workspace.input_data.drug_file_chunk\")\n",
    "        \n",
    "            print(f\"Inserted {len(chunks)} chunks from {file_name}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping existing file: {file} as it has been already chunked\")\n",
    "\n",
    "print(\"All files processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7808c7c5-bec5-4523-beba-abefc8d8772c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6893931041945264,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pdf_to_text_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
